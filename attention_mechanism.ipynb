{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJQapHV3/uPQrR1TJ6bxUQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrdbourke/learn-transformers/blob/main/attention_mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [WIP] Attention mechanism \n",
        "\n",
        "**Focus:** Build intuition to build up to replicating the original Transformer paper.\n",
        "\n",
        "### This notebook\n",
        "\n",
        "* Recreate self-attention as per Transformer paper\n",
        "* Recreate multi-head attention as per Transformer paper\n",
        "\n",
        "### Later\n",
        "* Recreate Transformer model architecture\n",
        "* Train on a simple example \n",
        "\n",
        "Sources:\n",
        "\n",
        "* Transformer paper: https://arxiv.org/abs/1706.03762\n",
        "* The annotated transformer: http://nlp.seas.harvard.edu/2018/04/01/attention.html \n",
        "* https://lilianweng.github.io/posts/2018-06-24-attention/#self-attention\n",
        "* https://jaykmody.com/blog/attention-intuition/\n",
        "* Compact transformers - https://medium.com/pytorch/training-compact-transformers-from-scratch-in-30-minutes-with-pytorch-ff5c21668ed5 "
      ],
      "metadata": {
        "id": "usdE3sRsGnj_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "6-XaYYv9GcKP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple scaled-dot-product-attention (no mask) \n",
        "\n",
        "TK:\n",
        "- Can I replicate this in Google Sheets?\n",
        "- Turn this function into the same format as the transformer paper (e.g. figure 2)"
      ],
      "metadata": {
        "id": "RvO_nyA2O6tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(query, key, value):  \n",
        "  d_k = torch.tensor(query.shape[-1]) # torch.sqrt needs a tensor\n",
        "  q_k = F.softmax(torch.matmul(query, key.T)/torch.sqrt(d_k), dim=-1)\n",
        "  return torch.matmul(q_k, value.T)"
      ],
      "metadata": {
        "id": "nMVXnybZGhun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "x = torch.randn(10, 10)\n",
        "\n",
        "attention(query=x, key=x, value=x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWgiQ8HWHjcK",
        "outputId": "29ef9a30-1004-40ea-e09c-fc7ae4949ebf"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.8933, -0.3887, -0.7377, -0.2461, -1.5391,  0.0764, -0.0861, -1.0471,\n",
              "          0.0089,  0.1876],\n",
              "        [ 0.4219, -0.7769,  0.8384,  0.3348,  0.0515,  0.2086,  1.2753,  0.2287,\n",
              "         -0.2691, -0.0226],\n",
              "        [ 0.4126, -0.0101,  0.7777, -0.7219, -0.8859, -0.6319,  0.0446, -0.0064,\n",
              "         -0.4851, -0.0692],\n",
              "        [-0.5196, -0.3668,  1.1495, -0.2994, -0.7661,  0.0208,  1.2501,  0.0915,\n",
              "         -0.0215,  0.3415],\n",
              "        [ 0.2543, -0.6703,  1.2743, -0.2598, -1.0734, -0.5371,  1.3273,  0.0430,\n",
              "          0.4069,  0.1190],\n",
              "        [-0.1206,  0.2336,  0.8385,  0.4162,  0.3648, -0.2610,  0.5973, -0.0789,\n",
              "          0.2875,  0.0406],\n",
              "        [-0.0315,  1.2026,  0.6622,  0.0972, -1.1233, -1.1447,  1.7388,  0.3997,\n",
              "          0.6841, -0.7225],\n",
              "        [-0.6842, -0.1752,  1.0953, -0.3012, -0.6531, -0.2085,  0.7526, -0.3217,\n",
              "         -0.1753,  0.0630],\n",
              "        [-0.0737, -0.3024,  0.6222,  0.0706, -0.5312, -0.0393,  0.9918, -0.3184,\n",
              "         -0.1715,  0.0771],\n",
              "        [ 0.2698,  0.1422,  0.6674,  0.0506, -0.0081, -0.1161,  0.3864, -0.2013,\n",
              "          0.1447,  0.1315]])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.size(-1)"
      ],
      "metadata": {
        "id": "iPUm3PTpHnhN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b3239c-7e3a-4ef3-a9c7-83a9017f197a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Simple scaled-dot-product-attention (with mask) \n",
        "\n",
        "UPTOHERE:\n",
        "* see: https://jaykmody.com/blog/gpt-from-scratch/#causal \n",
        "* And see: https://github.com/facebookresearch/xformers/blob/main/xformers/components/attention/attention_mask.py \n",
        "  * Default to causal mask: https://github.com/facebookresearch/xformers/blob/97daac83cece6d3d77bb09479777ad6e8ef7dfed/xformers/components/attention/attention_mask.py#LL74C16-L74C16 (`make_causal()`) "
      ],
      "metadata": {
        "id": "2xKawhtOcYfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make causal mask, see: https://jaykmody.com/blog/gpt-from-scratch/#causal\n",
        "additive_mask = torch.triu(\n",
        "    # torch.ones(x.shape[0], x.shape[0]) * float(\"inf\"),\n",
        "    torch.ones(x.shape[0], x.shape[0]) * -1e10, # can use -1e10 to prevent nans\n",
        "    diagonal=1\n",
        ")\n",
        "\n",
        "additive_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-66nS-Rdp0ox",
        "outputId": "673420c9-90c3-447e-d6da-f20b4353e3d2"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000e+00, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10,\n",
              "         -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
              "        [ 0.0000e+00,  0.0000e+00, -1.0000e+10, -1.0000e+10, -1.0000e+10,\n",
              "         -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+10, -1.0000e+10,\n",
              "         -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+10,\n",
              "         -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "          0.0000e+00, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+10, -1.0000e+10],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+10],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_with_mask(query, key, value, mask=None):  \n",
        "  d_k = torch.tensor(query.shape[-1]) # torch.sqrt needs a tensor\n",
        "  q_k = torch.matmul(query, key.T) / torch.sqrt(d_k)\n",
        "  print(q_k.shape)\n",
        "\n",
        "  \n",
        "  print(f\"q_k: {q_k}\")\n",
        "\n",
        "  # Apply attention mask\n",
        "  if mask is not None:\n",
        "    q_k = q_k + mask\n",
        "\n",
        "  print(f\"q_k with mask: {q_k}\")\n",
        "\n",
        "  # Softmax\n",
        "  attn = F.softmax(q_k, dim=-1)\n",
        "\n",
        "  return torch.matmul(attn, value), attn\n",
        "\n",
        "attention_with_mask(query=x, key=x, value=x, mask=additive_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJcxus6qcYRE",
        "outputId": "412a7da1-5a93-4047-b0e1-11c8f492b542"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 10])\n",
            "torch.Size([10, 10])\n",
            "q_k: tensor([[ 6.0130e+00, -7.9063e-01, -1.6862e+00, -5.4706e-01, -1.2946e+00,\n",
            "         -1.5651e-01, -9.5597e-01, -1.4247e-01,  4.3972e-01, -5.1831e-01],\n",
            "        [-7.9063e-01,  2.3093e+00, -5.7266e-01,  6.2206e-01,  3.1750e-01,\n",
            "         -9.4774e-01,  1.2001e-01,  1.7332e-01,  1.3094e+00, -7.1141e-01],\n",
            "        [-1.6862e+00, -5.7266e-01,  3.3930e+00,  3.3083e-03,  1.3948e-01,\n",
            "         -2.0294e-01,  2.6444e+00,  1.1129e+00, -1.9716e-01,  2.9590e-01],\n",
            "        [-5.4706e-01,  6.2206e-01,  3.3083e-03,  2.3570e+00,  2.2022e+00,\n",
            "         -6.3796e-01,  1.2451e+00,  7.0473e-01,  8.8856e-01, -9.7713e-01],\n",
            "        [-1.2946e+00,  3.1750e-01,  1.3948e-01,  2.2022e+00,  4.3364e+00,\n",
            "          2.9038e-01, -3.1773e-01,  1.8019e+00,  2.3513e-01, -5.8754e-01],\n",
            "        [-1.5651e-01, -9.4774e-01, -2.0294e-01, -6.3796e-01,  2.9038e-01,\n",
            "          1.7630e+00, -5.9041e-01, -8.1987e-02, -7.1263e-01,  1.0189e+00],\n",
            "        [-9.5597e-01,  1.2001e-01,  2.6444e+00,  1.2451e+00, -3.1773e-01,\n",
            "         -5.9041e-01,  4.7130e+00,  1.3152e+00,  1.0626e+00, -5.1111e-01],\n",
            "        [-1.4247e-01,  1.7332e-01,  1.1129e+00,  7.0473e-01,  1.8019e+00,\n",
            "         -8.1987e-02,  1.3152e+00,  2.9753e+00,  5.4820e-01, -6.4669e-01],\n",
            "        [ 4.3972e-01,  1.3094e+00, -1.9716e-01,  8.8856e-01,  2.3513e-01,\n",
            "         -7.1263e-01,  1.0626e+00,  5.4820e-01,  1.6245e+00, -6.7865e-01],\n",
            "        [-5.1831e-01, -7.1141e-01,  2.9590e-01, -9.7713e-01, -5.8754e-01,\n",
            "          1.0189e+00, -5.1111e-01, -6.4669e-01, -6.7865e-01,  1.0872e+00]])\n",
            "q_k with mask: tensor([[ 6.0130e+00, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10,\n",
            "         -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
            "        [-7.9063e-01,  2.3093e+00, -1.0000e+10, -1.0000e+10, -1.0000e+10,\n",
            "         -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
            "        [-1.6862e+00, -5.7266e-01,  3.3930e+00, -1.0000e+10, -1.0000e+10,\n",
            "         -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
            "        [-5.4706e-01,  6.2206e-01,  3.3083e-03,  2.3570e+00, -1.0000e+10,\n",
            "         -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
            "        [-1.2946e+00,  3.1750e-01,  1.3948e-01,  2.2022e+00,  4.3364e+00,\n",
            "         -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
            "        [-1.5651e-01, -9.4774e-01, -2.0294e-01, -6.3796e-01,  2.9038e-01,\n",
            "          1.7630e+00, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
            "        [-9.5597e-01,  1.2001e-01,  2.6444e+00,  1.2451e+00, -3.1773e-01,\n",
            "         -5.9041e-01,  4.7130e+00, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n",
            "        [-1.4247e-01,  1.7332e-01,  1.1129e+00,  7.0473e-01,  1.8019e+00,\n",
            "         -8.1987e-02,  1.3152e+00,  2.9753e+00, -1.0000e+10, -1.0000e+10],\n",
            "        [ 4.3972e-01,  1.3094e+00, -1.9716e-01,  8.8856e-01,  2.3513e-01,\n",
            "         -7.1263e-01,  1.0626e+00,  5.4820e-01,  1.6245e+00, -1.0000e+10],\n",
            "        [-5.1831e-01, -7.1141e-01,  2.9590e-01, -9.7713e-01, -5.8754e-01,\n",
            "          1.0189e+00, -5.1111e-01, -6.4669e-01, -6.7865e-01,  1.0872e+00]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047,\n",
              "          -0.7521,  1.6487],\n",
              "         [-0.2925, -1.2790, -0.6577, -0.6261, -0.7064,  0.6764,  1.5697, -0.2219,\n",
              "          -0.5084,  0.4917],\n",
              "         [-0.7351,  1.0349,  0.7731,  1.6162,  1.2376,  1.2712,  0.6256,  1.2893,\n",
              "          -0.2397,  0.0589],\n",
              "         [-0.2166,  0.6004, -1.0463, -0.6979, -0.1510,  1.4382,  0.5008, -0.3120,\n",
              "           0.1167, -0.4545],\n",
              "         [-1.3844,  0.9470, -0.9017, -0.6031, -1.1193,  2.0389, -1.0030, -0.4560,\n",
              "          -0.7730, -0.6367],\n",
              "         [-0.0906,  0.6622, -0.3702,  0.5163, -0.5373, -0.0254, -0.8781, -0.1037,\n",
              "          -0.2517,  0.4371],\n",
              "         [-0.1764,  1.6977, -0.9631,  1.3173,  1.3377,  0.9195,  1.9504,  0.5652,\n",
              "           0.2647, -0.1750],\n",
              "         [-0.8553,  1.1510, -0.3746,  0.3669,  0.0303,  0.8280,  0.4054, -0.3187,\n",
              "          -1.3058, -0.4890],\n",
              "         [-0.1844,  0.3723, -0.7960, -0.2162,  0.2121,  0.7338,  0.8952, -0.2755,\n",
              "          -0.4961,  0.1444],\n",
              "         [-0.0799,  0.4215, -0.1679,  0.7412, -0.0078,  0.1213, -0.3360,  0.0507,\n",
              "          -0.1702,  0.3801]]),\n",
              " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000],\n",
              "         [0.0431, 0.9569, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000],\n",
              "         [0.0061, 0.0185, 0.9754, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000],\n",
              "         [0.0413, 0.1330, 0.0716, 0.7540, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000],\n",
              "         [0.0031, 0.0156, 0.0130, 0.1025, 0.8659, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000],\n",
              "         [0.0877, 0.0397, 0.0837, 0.0542, 0.1371, 0.5977, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000],\n",
              "         [0.0029, 0.0086, 0.1068, 0.0264, 0.0055, 0.0042, 0.8456, 0.0000, 0.0000,\n",
              "          0.0000],\n",
              "         [0.0232, 0.0318, 0.0813, 0.0541, 0.1619, 0.0246, 0.0995, 0.5236, 0.0000,\n",
              "          0.0000],\n",
              "         [0.0778, 0.1855, 0.0411, 0.1218, 0.0634, 0.0246, 0.1449, 0.0867, 0.2543,\n",
              "          0.0000],\n",
              "         [0.0555, 0.0458, 0.1253, 0.0351, 0.0518, 0.2582, 0.0559, 0.0488, 0.0473,\n",
              "          0.2764]]))"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "A1EUhnZ-fWha",
        "outputId": "e9d0aee8-0eb6-4994-c46d-6db717828b08"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 10])\n",
            "torch.Size([10, 10])\n",
            "tensor([[ 6.0130e+00, -7.9063e-01, -1.6862e+00, -5.4706e-01, -1.2946e+00,\n",
            "         -1.5651e-01, -9.5597e-01, -1.4247e-01,  4.3972e-01, -5.1831e-01],\n",
            "        [-7.9063e-01,  2.3093e+00, -5.7266e-01,  6.2206e-01,  3.1750e-01,\n",
            "         -9.4774e-01,  1.2001e-01,  1.7332e-01,  1.3094e+00, -7.1141e-01],\n",
            "        [-1.6862e+00, -5.7266e-01,  3.3930e+00,  3.3083e-03,  1.3948e-01,\n",
            "         -2.0294e-01,  2.6444e+00,  1.1129e+00, -1.9716e-01,  2.9590e-01],\n",
            "        [-5.4706e-01,  6.2206e-01,  3.3083e-03,  2.3570e+00,  2.2022e+00,\n",
            "         -6.3796e-01,  1.2451e+00,  7.0473e-01,  8.8856e-01, -9.7713e-01],\n",
            "        [-1.2946e+00,  3.1750e-01,  1.3948e-01,  2.2022e+00,  4.3364e+00,\n",
            "          2.9038e-01, -3.1773e-01,  1.8019e+00,  2.3513e-01, -5.8754e-01],\n",
            "        [-1.5651e-01, -9.4774e-01, -2.0294e-01, -6.3796e-01,  2.9038e-01,\n",
            "          1.7630e+00, -5.9041e-01, -8.1987e-02, -7.1263e-01,  1.0189e+00],\n",
            "        [-9.5597e-01,  1.2001e-01,  2.6444e+00,  1.2451e+00, -3.1773e-01,\n",
            "         -5.9041e-01,  4.7130e+00,  1.3152e+00,  1.0626e+00, -5.1111e-01],\n",
            "        [-1.4247e-01,  1.7332e-01,  1.1129e+00,  7.0473e-01,  1.8019e+00,\n",
            "         -8.1987e-02,  1.3152e+00,  2.9753e+00,  5.4820e-01, -6.4669e-01],\n",
            "        [ 4.3972e-01,  1.3094e+00, -1.9716e-01,  8.8856e-01,  2.3513e-01,\n",
            "         -7.1263e-01,  1.0626e+00,  5.4820e-01,  1.6245e+00, -6.7865e-01],\n",
            "        [-5.1831e-01, -7.1141e-01,  2.9590e-01, -9.7713e-01, -5.8754e-01,\n",
            "          1.0189e+00, -5.1111e-01, -6.4669e-01, -6.7865e-01,  1.0872e+00]])\n",
            "tensor([[ True, False, False, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False, False, False, False, False, False, False, False],\n",
            "        [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "        [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "        [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "        [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
            "        [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-11d3489d4165>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mattention_with_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-90-81ec761bc3b4>\u001b[0m in \u001b[0;36mattention_with_mask\u001b[0;34m(query, key, value, mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# Apply attention mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mq_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Why scaled?\n",
        "\n",
        "TL;DR softmax can get out of hand with large values"
      ],
      "metadata": {
        "id": "Y-c_uyAVUDzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_values = torch.tensor([1, 2, 3], dtype=torch.float32) # need dtype otherwise error\n",
        "big_values = small_values * 10 \n",
        "huge_values = big_values * 10\n",
        "\n",
        "small_softmax = F.softmax(small_values, dim=0)\n",
        "big_softmax = F.softmax(big_values, dim=0)\n",
        "huge_softmax = F.softmax(huge_values, dim=0)\n",
        "\n",
        "print(f\"Small values: {small_values}\\nSmall softmax: {small_softmax}\\n\")\n",
        "print(f\"Big values: {big_values}\\nBig softmax: {big_softmax}\\n\")\n",
        "print(f\"Huge values: {huge_values}\\nHuge softmax: {huge_softmax}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9ZRWyJTUJUx",
        "outputId": "00049a3b-4cbd-47d6-f749-c20eca57c686"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small values: tensor([1., 2., 3.])\n",
            "Small softmax: tensor([0.0900, 0.2447, 0.6652])\n",
            "\n",
            "Big values: tensor([10., 20., 30.])\n",
            "Big softmax: tensor([2.0611e-09, 4.5398e-05, 9.9995e-01])\n",
            "\n",
            "Huge values: tensor([100., 200., 300.])\n",
            "Huge softmax: tensor([0.0000e+00, 3.7835e-44, 1.0000e+00])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Why dot-product?\n",
        "\n",
        "TL;DR dot product measures how closely two vectors are related \n",
        "\n",
        "* big values = close\n",
        "* negative values = far away\n",
        "* zero value = same direction? (TK - fix this)\n",
        "\n",
        "See:\n",
        "* 3blue1brown on dot product - https://www.youtube.com/watch?v=LyGKycYT2v0"
      ],
      "metadata": {
        "id": "lQV-oF1SUm8u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hw3orswBU-GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Replicate PyTorch's `scaled_dot_product_attention` \n",
        "\n",
        "(minus all the fancy optimizations, the library can do those for us)\n",
        "\n",
        "See: https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "\n",
        "Also see: https://github.com/facebookresearch/xformers/blob/main/xformers/components/attention/core.py#L297 "
      ],
      "metadata": {
        "id": "Mi9DKRGiOQJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally use the context manager to ensure one of the fused kerenels is run\n",
        "query = torch.rand(32, 8, 128, 64)\n",
        "key = torch.rand(32, 8, 128, 64)\n",
        "value = torch.rand(32, 8, 128, 64)\n",
        "\n",
        "F.scaled_dot_product_attention(query,key,value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDOCXshXQPxA",
        "outputId": "556ad25f-b825-4013-e2fe-6b096f1d9476"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.5064, 0.5570, 0.4984,  ..., 0.4607, 0.5188, 0.5203],\n",
              "          [0.5054, 0.5610, 0.4979,  ..., 0.4616, 0.5145, 0.5213],\n",
              "          [0.5082, 0.5588, 0.4970,  ..., 0.4598, 0.5156, 0.5171],\n",
              "          ...,\n",
              "          [0.5053, 0.5617, 0.4959,  ..., 0.4591, 0.5203, 0.5212],\n",
              "          [0.5072, 0.5619, 0.4990,  ..., 0.4625, 0.5145, 0.5201],\n",
              "          [0.5055, 0.5632, 0.4981,  ..., 0.4622, 0.5161, 0.5197]],\n",
              "\n",
              "         [[0.4851, 0.5138, 0.5436,  ..., 0.4788, 0.5471, 0.4800],\n",
              "          [0.4856, 0.5199, 0.5413,  ..., 0.4726, 0.5412, 0.4804],\n",
              "          [0.4879, 0.5136, 0.5421,  ..., 0.4744, 0.5447, 0.4798],\n",
              "          ...,\n",
              "          [0.4885, 0.5149, 0.5462,  ..., 0.4739, 0.5466, 0.4794],\n",
              "          [0.4873, 0.5157, 0.5460,  ..., 0.4710, 0.5407, 0.4825],\n",
              "          [0.4868, 0.5140, 0.5460,  ..., 0.4763, 0.5440, 0.4813]],\n",
              "\n",
              "         [[0.5155, 0.4867, 0.4495,  ..., 0.5153, 0.4683, 0.4834],\n",
              "          [0.5125, 0.4893, 0.4498,  ..., 0.5186, 0.4674, 0.4860],\n",
              "          [0.5111, 0.4905, 0.4516,  ..., 0.5177, 0.4677, 0.4784],\n",
              "          ...,\n",
              "          [0.5125, 0.4903, 0.4520,  ..., 0.5163, 0.4669, 0.4818],\n",
              "          [0.5094, 0.4892, 0.4547,  ..., 0.5190, 0.4694, 0.4820],\n",
              "          [0.5103, 0.4914, 0.4553,  ..., 0.5168, 0.4701, 0.4833]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[0.5099, 0.4647, 0.4930,  ..., 0.5122, 0.4851, 0.4972],\n",
              "          [0.5095, 0.4646, 0.4926,  ..., 0.5123, 0.4851, 0.4970],\n",
              "          [0.5085, 0.4601, 0.4927,  ..., 0.5127, 0.4873, 0.4974],\n",
              "          ...,\n",
              "          [0.5079, 0.4619, 0.4984,  ..., 0.5105, 0.4856, 0.4956],\n",
              "          [0.5081, 0.4643, 0.4985,  ..., 0.5135, 0.4847, 0.4982],\n",
              "          [0.5101, 0.4569, 0.4909,  ..., 0.5139, 0.4861, 0.4959]],\n",
              "\n",
              "         [[0.4964, 0.5401, 0.4555,  ..., 0.5513, 0.5094, 0.4901],\n",
              "          [0.4920, 0.5417, 0.4582,  ..., 0.5537, 0.5112, 0.4879],\n",
              "          [0.4936, 0.5359, 0.4575,  ..., 0.5480, 0.5139, 0.4903],\n",
              "          ...,\n",
              "          [0.4991, 0.5422, 0.4525,  ..., 0.5500, 0.5087, 0.4900],\n",
              "          [0.4989, 0.5393, 0.4578,  ..., 0.5497, 0.5119, 0.4898],\n",
              "          [0.4932, 0.5380, 0.4583,  ..., 0.5521, 0.5110, 0.4902]],\n",
              "\n",
              "         [[0.4869, 0.5030, 0.4608,  ..., 0.5216, 0.5097, 0.4668],\n",
              "          [0.4889, 0.5051, 0.4615,  ..., 0.5180, 0.5127, 0.4674],\n",
              "          [0.4868, 0.5018, 0.4650,  ..., 0.5217, 0.5117, 0.4674],\n",
              "          ...,\n",
              "          [0.4888, 0.5030, 0.4611,  ..., 0.5232, 0.5082, 0.4679],\n",
              "          [0.4892, 0.5035, 0.4641,  ..., 0.5192, 0.5091, 0.4674],\n",
              "          [0.4883, 0.5023, 0.4617,  ..., 0.5197, 0.5093, 0.4673]]],\n",
              "\n",
              "\n",
              "        [[[0.4651, 0.4678, 0.4957,  ..., 0.5215, 0.4990, 0.4809],\n",
              "          [0.4661, 0.4695, 0.4910,  ..., 0.5178, 0.4985, 0.4857],\n",
              "          [0.4658, 0.4716, 0.4946,  ..., 0.5187, 0.4971, 0.4814],\n",
              "          ...,\n",
              "          [0.4654, 0.4700, 0.4935,  ..., 0.5214, 0.4976, 0.4792],\n",
              "          [0.4635, 0.4706, 0.4940,  ..., 0.5209, 0.4976, 0.4830],\n",
              "          [0.4656, 0.4654, 0.4924,  ..., 0.5210, 0.5034, 0.4839]],\n",
              "\n",
              "         [[0.4686, 0.5204, 0.4833,  ..., 0.5248, 0.4531, 0.4794],\n",
              "          [0.4656, 0.5251, 0.4791,  ..., 0.5269, 0.4533, 0.4773],\n",
              "          [0.4667, 0.5261, 0.4829,  ..., 0.5199, 0.4505, 0.4743],\n",
              "          ...,\n",
              "          [0.4628, 0.5231, 0.4794,  ..., 0.5192, 0.4478, 0.4851],\n",
              "          [0.4639, 0.5253, 0.4839,  ..., 0.5205, 0.4505, 0.4772],\n",
              "          [0.4667, 0.5197, 0.4804,  ..., 0.5162, 0.4508, 0.4790]],\n",
              "\n",
              "         [[0.5107, 0.4723, 0.4960,  ..., 0.5156, 0.4930, 0.5081],\n",
              "          [0.5159, 0.4739, 0.5009,  ..., 0.5151, 0.4923, 0.5073],\n",
              "          [0.5128, 0.4700, 0.4988,  ..., 0.5171, 0.4900, 0.5136],\n",
              "          ...,\n",
              "          [0.5102, 0.4727, 0.4987,  ..., 0.5137, 0.4902, 0.5107],\n",
              "          [0.5142, 0.4711, 0.5001,  ..., 0.5097, 0.4869, 0.5151],\n",
              "          [0.5135, 0.4726, 0.5009,  ..., 0.5157, 0.4904, 0.5132]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[0.5231, 0.5025, 0.4902,  ..., 0.5003, 0.4911, 0.5276],\n",
              "          [0.5269, 0.5022, 0.4953,  ..., 0.5032, 0.4951, 0.5256],\n",
              "          [0.5289, 0.4990, 0.4902,  ..., 0.4988, 0.4904, 0.5284],\n",
              "          ...,\n",
              "          [0.5240, 0.5016, 0.4895,  ..., 0.4997, 0.4909, 0.5266],\n",
              "          [0.5249, 0.4983, 0.4941,  ..., 0.4998, 0.4930, 0.5239],\n",
              "          [0.5302, 0.5010, 0.4909,  ..., 0.4999, 0.4955, 0.5244]],\n",
              "\n",
              "         [[0.5265, 0.5181, 0.5155,  ..., 0.5040, 0.5057, 0.4696],\n",
              "          [0.5257, 0.5209, 0.5146,  ..., 0.5114, 0.5045, 0.4638],\n",
              "          [0.5240, 0.5214, 0.5090,  ..., 0.5014, 0.5040, 0.4642],\n",
              "          ...,\n",
              "          [0.5242, 0.5197, 0.5103,  ..., 0.5046, 0.5015, 0.4656],\n",
              "          [0.5259, 0.5231, 0.5127,  ..., 0.5085, 0.5027, 0.4613],\n",
              "          [0.5242, 0.5241, 0.5151,  ..., 0.5014, 0.5051, 0.4651]],\n",
              "\n",
              "         [[0.5000, 0.4895, 0.4856,  ..., 0.4952, 0.5262, 0.4252],\n",
              "          [0.5013, 0.4921, 0.4893,  ..., 0.4977, 0.5247, 0.4284],\n",
              "          [0.5022, 0.4911, 0.4904,  ..., 0.4968, 0.5223, 0.4277],\n",
              "          ...,\n",
              "          [0.5049, 0.4916, 0.4890,  ..., 0.4958, 0.5204, 0.4280],\n",
              "          [0.5012, 0.4901, 0.4872,  ..., 0.4973, 0.5221, 0.4288],\n",
              "          [0.4977, 0.4889, 0.4872,  ..., 0.4945, 0.5252, 0.4257]]],\n",
              "\n",
              "\n",
              "        [[[0.4691, 0.5220, 0.4606,  ..., 0.4974, 0.5121, 0.5211],\n",
              "          [0.4668, 0.5176, 0.4603,  ..., 0.4985, 0.5168, 0.5148],\n",
              "          [0.4646, 0.5240, 0.4591,  ..., 0.4958, 0.5140, 0.5218],\n",
              "          ...,\n",
              "          [0.4671, 0.5228, 0.4587,  ..., 0.4990, 0.5146, 0.5209],\n",
              "          [0.4686, 0.5190, 0.4577,  ..., 0.4985, 0.5165, 0.5225],\n",
              "          [0.4623, 0.5182, 0.4590,  ..., 0.4974, 0.5115, 0.5202]],\n",
              "\n",
              "         [[0.4684, 0.4880, 0.5202,  ..., 0.5043, 0.4671, 0.5415],\n",
              "          [0.4709, 0.4914, 0.5185,  ..., 0.4987, 0.4662, 0.5417],\n",
              "          [0.4693, 0.4900, 0.5199,  ..., 0.5013, 0.4672, 0.5377],\n",
              "          ...,\n",
              "          [0.4688, 0.4872, 0.5189,  ..., 0.5016, 0.4679, 0.5374],\n",
              "          [0.4647, 0.4859, 0.5138,  ..., 0.5069, 0.4654, 0.5421],\n",
              "          [0.4689, 0.4874, 0.5176,  ..., 0.5020, 0.4667, 0.5419]],\n",
              "\n",
              "         [[0.5199, 0.4703, 0.4519,  ..., 0.4858, 0.4513, 0.5233],\n",
              "          [0.5234, 0.4761, 0.4498,  ..., 0.4867, 0.4516, 0.5234],\n",
              "          [0.5251, 0.4745, 0.4541,  ..., 0.4911, 0.4514, 0.5234],\n",
              "          ...,\n",
              "          [0.5235, 0.4708, 0.4501,  ..., 0.4829, 0.4478, 0.5185],\n",
              "          [0.5219, 0.4736, 0.4560,  ..., 0.4884, 0.4517, 0.5171],\n",
              "          [0.5248, 0.4746, 0.4513,  ..., 0.4854, 0.4522, 0.5219]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[0.4901, 0.4783, 0.5250,  ..., 0.5096, 0.5633, 0.5208],\n",
              "          [0.4954, 0.4817, 0.5246,  ..., 0.5103, 0.5591, 0.5200],\n",
              "          [0.4905, 0.4810, 0.5218,  ..., 0.5146, 0.5600, 0.5243],\n",
              "          ...,\n",
              "          [0.4930, 0.4808, 0.5260,  ..., 0.5126, 0.5658, 0.5208],\n",
              "          [0.4906, 0.4845, 0.5244,  ..., 0.5139, 0.5590, 0.5195],\n",
              "          [0.4926, 0.4804, 0.5235,  ..., 0.5117, 0.5643, 0.5157]],\n",
              "\n",
              "         [[0.5192, 0.4938, 0.4802,  ..., 0.5025, 0.4730, 0.4476],\n",
              "          [0.5215, 0.4904, 0.4781,  ..., 0.4975, 0.4746, 0.4502],\n",
              "          [0.5183, 0.4890, 0.4824,  ..., 0.4963, 0.4727, 0.4498],\n",
              "          ...,\n",
              "          [0.5175, 0.4914, 0.4766,  ..., 0.4993, 0.4739, 0.4484],\n",
              "          [0.5180, 0.4908, 0.4795,  ..., 0.4959, 0.4768, 0.4494],\n",
              "          [0.5199, 0.4919, 0.4769,  ..., 0.4990, 0.4729, 0.4481]],\n",
              "\n",
              "         [[0.5363, 0.4820, 0.5025,  ..., 0.5316, 0.5062, 0.4802],\n",
              "          [0.5354, 0.4898, 0.5049,  ..., 0.5372, 0.5107, 0.4828],\n",
              "          [0.5392, 0.4857, 0.5077,  ..., 0.5311, 0.5095, 0.4816],\n",
              "          ...,\n",
              "          [0.5367, 0.4855, 0.5015,  ..., 0.5318, 0.5037, 0.4798],\n",
              "          [0.5342, 0.4844, 0.5047,  ..., 0.5312, 0.5085, 0.4814],\n",
              "          [0.5366, 0.4841, 0.5059,  ..., 0.5353, 0.5090, 0.4854]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[0.4672, 0.5208, 0.5384,  ..., 0.5515, 0.4920, 0.4605],\n",
              "          [0.4684, 0.5180, 0.5369,  ..., 0.5571, 0.4857, 0.4617],\n",
              "          [0.4679, 0.5194, 0.5406,  ..., 0.5563, 0.4893, 0.4609],\n",
              "          ...,\n",
              "          [0.4694, 0.5197, 0.5380,  ..., 0.5556, 0.4851, 0.4603],\n",
              "          [0.4722, 0.5170, 0.5332,  ..., 0.5553, 0.4904, 0.4596],\n",
              "          [0.4699, 0.5182, 0.5381,  ..., 0.5529, 0.4888, 0.4586]],\n",
              "\n",
              "         [[0.5222, 0.5339, 0.5083,  ..., 0.5304, 0.5027, 0.4949],\n",
              "          [0.5206, 0.5311, 0.5082,  ..., 0.5291, 0.4999, 0.4963],\n",
              "          [0.5227, 0.5299, 0.5092,  ..., 0.5273, 0.5048, 0.4944],\n",
              "          ...,\n",
              "          [0.5202, 0.5320, 0.5085,  ..., 0.5284, 0.5032, 0.4966],\n",
              "          [0.5201, 0.5306, 0.5095,  ..., 0.5297, 0.5040, 0.4935],\n",
              "          [0.5245, 0.5309, 0.5114,  ..., 0.5325, 0.5047, 0.4942]],\n",
              "\n",
              "         [[0.4823, 0.5463, 0.5168,  ..., 0.5121, 0.4931, 0.5057],\n",
              "          [0.4814, 0.5393, 0.5171,  ..., 0.5119, 0.4993, 0.5076],\n",
              "          [0.4820, 0.5470, 0.5174,  ..., 0.5109, 0.4948, 0.5038],\n",
              "          ...,\n",
              "          [0.4853, 0.5476, 0.5163,  ..., 0.5130, 0.4875, 0.5060],\n",
              "          [0.4864, 0.5463, 0.5168,  ..., 0.5133, 0.4973, 0.5013],\n",
              "          [0.4849, 0.5463, 0.5177,  ..., 0.5114, 0.4940, 0.5023]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[0.5681, 0.4900, 0.4920,  ..., 0.5363, 0.4831, 0.5494],\n",
              "          [0.5696, 0.4884, 0.4976,  ..., 0.5327, 0.4839, 0.5543],\n",
              "          [0.5731, 0.4872, 0.4981,  ..., 0.5340, 0.4837, 0.5539],\n",
              "          ...,\n",
              "          [0.5701, 0.4868, 0.4898,  ..., 0.5346, 0.4855, 0.5559],\n",
              "          [0.5703, 0.4859, 0.4967,  ..., 0.5391, 0.4892, 0.5525],\n",
              "          [0.5745, 0.4897, 0.4917,  ..., 0.5394, 0.4853, 0.5505]],\n",
              "\n",
              "         [[0.5514, 0.5381, 0.5721,  ..., 0.5205, 0.5349, 0.5127],\n",
              "          [0.5503, 0.5421, 0.5733,  ..., 0.5202, 0.5334, 0.5123],\n",
              "          [0.5526, 0.5399, 0.5708,  ..., 0.5207, 0.5324, 0.5130],\n",
              "          ...,\n",
              "          [0.5533, 0.5420, 0.5710,  ..., 0.5176, 0.5330, 0.5121],\n",
              "          [0.5476, 0.5399, 0.5747,  ..., 0.5207, 0.5351, 0.5144],\n",
              "          [0.5523, 0.5408, 0.5731,  ..., 0.5204, 0.5360, 0.5132]],\n",
              "\n",
              "         [[0.4970, 0.5524, 0.5140,  ..., 0.4909, 0.5042, 0.4719],\n",
              "          [0.4931, 0.5533, 0.5109,  ..., 0.4917, 0.5006, 0.4741],\n",
              "          [0.4958, 0.5505, 0.5076,  ..., 0.4938, 0.5046, 0.4707],\n",
              "          ...,\n",
              "          [0.4977, 0.5503, 0.5102,  ..., 0.4912, 0.5080, 0.4742],\n",
              "          [0.4961, 0.5483, 0.5065,  ..., 0.4907, 0.4997, 0.4719],\n",
              "          [0.4988, 0.5546, 0.5094,  ..., 0.4911, 0.5015, 0.4680]]],\n",
              "\n",
              "\n",
              "        [[[0.5333, 0.5204, 0.4813,  ..., 0.4755, 0.5344, 0.5425],\n",
              "          [0.5340, 0.5185, 0.4822,  ..., 0.4795, 0.5409, 0.5402],\n",
              "          [0.5319, 0.5210, 0.4849,  ..., 0.4779, 0.5394, 0.5397],\n",
              "          ...,\n",
              "          [0.5368, 0.5180, 0.4829,  ..., 0.4772, 0.5413, 0.5397],\n",
              "          [0.5332, 0.5180, 0.4806,  ..., 0.4759, 0.5416, 0.5439],\n",
              "          [0.5330, 0.5157, 0.4784,  ..., 0.4790, 0.5400, 0.5403]],\n",
              "\n",
              "         [[0.5074, 0.4994, 0.4586,  ..., 0.4783, 0.5007, 0.4590],\n",
              "          [0.5076, 0.4987, 0.4571,  ..., 0.4781, 0.4977, 0.4612],\n",
              "          [0.5064, 0.4982, 0.4529,  ..., 0.4741, 0.5004, 0.4565],\n",
              "          ...,\n",
              "          [0.5091, 0.4994, 0.4502,  ..., 0.4795, 0.4996, 0.4605],\n",
              "          [0.5040, 0.4992, 0.4595,  ..., 0.4784, 0.4986, 0.4607],\n",
              "          [0.5052, 0.4999, 0.4527,  ..., 0.4818, 0.4971, 0.4629]],\n",
              "\n",
              "         [[0.4620, 0.5027, 0.4723,  ..., 0.5074, 0.5325, 0.4991],\n",
              "          [0.4675, 0.5013, 0.4780,  ..., 0.5067, 0.5268, 0.5036],\n",
              "          [0.4674, 0.5088, 0.4740,  ..., 0.5070, 0.5284, 0.5002],\n",
              "          ...,\n",
              "          [0.4669, 0.5024, 0.4752,  ..., 0.5030, 0.5244, 0.5056],\n",
              "          [0.4666, 0.5046, 0.4725,  ..., 0.5081, 0.5295, 0.4983],\n",
              "          [0.4683, 0.5021, 0.4821,  ..., 0.5044, 0.5258, 0.5041]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[0.4857, 0.4805, 0.4942,  ..., 0.4844, 0.4381, 0.5188],\n",
              "          [0.4831, 0.4784, 0.4940,  ..., 0.4816, 0.4367, 0.5148],\n",
              "          [0.4799, 0.4785, 0.4934,  ..., 0.4800, 0.4337, 0.5182],\n",
              "          ...,\n",
              "          [0.4855, 0.4786, 0.4931,  ..., 0.4783, 0.4365, 0.5162],\n",
              "          [0.4829, 0.4836, 0.4920,  ..., 0.4847, 0.4354, 0.5160],\n",
              "          [0.4820, 0.4795, 0.4934,  ..., 0.4804, 0.4365, 0.5161]],\n",
              "\n",
              "         [[0.4714, 0.4866, 0.5140,  ..., 0.5626, 0.5264, 0.4715],\n",
              "          [0.4733, 0.4816, 0.5073,  ..., 0.5614, 0.5225, 0.4655],\n",
              "          [0.4692, 0.4865, 0.5152,  ..., 0.5596, 0.5233, 0.4725],\n",
              "          ...,\n",
              "          [0.4716, 0.4860, 0.5099,  ..., 0.5610, 0.5242, 0.4650],\n",
              "          [0.4685, 0.4866, 0.5156,  ..., 0.5622, 0.5240, 0.4721],\n",
              "          [0.4716, 0.4833, 0.5129,  ..., 0.5616, 0.5222, 0.4717]],\n",
              "\n",
              "         [[0.4442, 0.5075, 0.5170,  ..., 0.4978, 0.5053, 0.5502],\n",
              "          [0.4394, 0.5073, 0.5195,  ..., 0.4960, 0.5024, 0.5517],\n",
              "          [0.4423, 0.5031, 0.5203,  ..., 0.4987, 0.4991, 0.5591],\n",
              "          ...,\n",
              "          [0.4402, 0.5048, 0.5181,  ..., 0.4980, 0.5009, 0.5499],\n",
              "          [0.4416, 0.5027, 0.5175,  ..., 0.4994, 0.5025, 0.5514],\n",
              "          [0.4408, 0.5055, 0.5177,  ..., 0.5005, 0.5027, 0.5506]]],\n",
              "\n",
              "\n",
              "        [[[0.5554, 0.5134, 0.5297,  ..., 0.5309, 0.4809, 0.5021],\n",
              "          [0.5524, 0.5132, 0.5350,  ..., 0.5319, 0.4821, 0.5012],\n",
              "          [0.5523, 0.5127, 0.5315,  ..., 0.5324, 0.4845, 0.5066],\n",
              "          ...,\n",
              "          [0.5519, 0.5149, 0.5340,  ..., 0.5304, 0.4857, 0.5023],\n",
              "          [0.5535, 0.5127, 0.5396,  ..., 0.5376, 0.4777, 0.5044],\n",
              "          [0.5510, 0.5167, 0.5376,  ..., 0.5345, 0.4791, 0.5047]],\n",
              "\n",
              "         [[0.5048, 0.5090, 0.5353,  ..., 0.5064, 0.5100, 0.5284],\n",
              "          [0.5035, 0.5112, 0.5336,  ..., 0.5009, 0.5114, 0.5265],\n",
              "          [0.4989, 0.5079, 0.5300,  ..., 0.5058, 0.5153, 0.5280],\n",
              "          ...,\n",
              "          [0.5087, 0.5086, 0.5332,  ..., 0.5035, 0.5075, 0.5278],\n",
              "          [0.5053, 0.5085, 0.5359,  ..., 0.5039, 0.5094, 0.5286],\n",
              "          [0.5057, 0.5067, 0.5329,  ..., 0.5035, 0.5101, 0.5285]],\n",
              "\n",
              "         [[0.4939, 0.5313, 0.5302,  ..., 0.4932, 0.4531, 0.4668],\n",
              "          [0.4954, 0.5314, 0.5312,  ..., 0.4941, 0.4582, 0.4622],\n",
              "          [0.4966, 0.5327, 0.5318,  ..., 0.4949, 0.4572, 0.4603],\n",
              "          ...,\n",
              "          [0.4944, 0.5264, 0.5300,  ..., 0.4933, 0.4534, 0.4616],\n",
              "          [0.4969, 0.5317, 0.5280,  ..., 0.4892, 0.4530, 0.4631],\n",
              "          [0.4957, 0.5305, 0.5329,  ..., 0.4950, 0.4577, 0.4612]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[0.5173, 0.4787, 0.5147,  ..., 0.5174, 0.4922, 0.4611],\n",
              "          [0.5177, 0.4710, 0.5128,  ..., 0.5212, 0.4930, 0.4706],\n",
              "          [0.5160, 0.4751, 0.5176,  ..., 0.5198, 0.5011, 0.4617],\n",
              "          ...,\n",
              "          [0.5212, 0.4794, 0.5134,  ..., 0.5202, 0.4936, 0.4650],\n",
              "          [0.5173, 0.4787, 0.5132,  ..., 0.5206, 0.4913, 0.4675],\n",
              "          [0.5140, 0.4800, 0.5140,  ..., 0.5167, 0.4915, 0.4649]],\n",
              "\n",
              "         [[0.5069, 0.5366, 0.4883,  ..., 0.5114, 0.5315, 0.5057],\n",
              "          [0.5034, 0.5360, 0.4868,  ..., 0.5158, 0.5318, 0.5089],\n",
              "          [0.4990, 0.5340, 0.4858,  ..., 0.5143, 0.5317, 0.5114],\n",
              "          ...,\n",
              "          [0.5004, 0.5367, 0.4869,  ..., 0.5141, 0.5314, 0.5092],\n",
              "          [0.5001, 0.5359, 0.4861,  ..., 0.5101, 0.5309, 0.5071],\n",
              "          [0.5010, 0.5335, 0.4880,  ..., 0.5140, 0.5303, 0.5100]],\n",
              "\n",
              "         [[0.5363, 0.4821, 0.4347,  ..., 0.5071, 0.5356, 0.5397],\n",
              "          [0.5365, 0.4810, 0.4360,  ..., 0.5067, 0.5423, 0.5418],\n",
              "          [0.5354, 0.4773, 0.4398,  ..., 0.5045, 0.5431, 0.5429],\n",
              "          ...,\n",
              "          [0.5366, 0.4837, 0.4378,  ..., 0.5073, 0.5404, 0.5415],\n",
              "          [0.5358, 0.4826, 0.4376,  ..., 0.5106, 0.5400, 0.5428],\n",
              "          [0.5367, 0.4873, 0.4340,  ..., 0.5098, 0.5369, 0.5415]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Why multi-head attention?\n",
        "\n",
        "TL;DR more opportunities to learn (e.g. 8x64 scaled dot-product attention = better than 1*512)\n",
        "\n",
        "TK:\n",
        "- One big matrix multiplication better than lots of small ones\n",
        "- Just perform a `nn.Linear()` then break it up"
      ],
      "metadata": {
        "id": "81KIT2MtYKyH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "InUnaU5hYY4i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}